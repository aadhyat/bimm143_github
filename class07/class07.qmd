---
title: "Class 7: Machine Learning 1"
author: "Aadhya Tripathi (PID: A17878439)"
toc: true
format: pdf
---

## Background
Today we will begin our exploration of some important machine learning methods, namely **clustering** and **dimensionality reduction**.

Let's make up some input data for clustering where we know what the natural "clusters" are.

The function `rnorm()` can be useful here.

```{r}
hist(rnorm(5000))
```
> Q. Generate 30 random numbers centered at +3, and another 30 centered at -3.

```{r}
tmp <- c(rnorm(30, mean = 3),
         rnorm(30, mean = -3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)

# rev - reverses vector order (ex. A to Z becomes Z to A)
# cbind - combines two vectors into two columns
```

## K-means clustering

The main function in "base R" for K-means clustering is called `kmeans()`:

```{r}
km <- kmeans(x, centers=2)
km
```

> Q. What components of the results object details the cluster sizes?

```{r}
km$size
```

> Q. What components of the results object details the cluster centers?

```{r}
km$centers
```

> Q. What components of the results object details the cluster membership vector (the main results of which points lie in which cluster)?

```{r}
km$cluster
```

> Q. Plot our clustering results with points colored by cluster membership. Also add the cluster centers as new points colored blue.

```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=15)
```

> Q. Run `kmeans()` again and this time produce 4 clusters. Call your result object `k4`.

```{r}
k4 <- kmeans(x, centers=4)
k4$cluster
plot(x, col=k4$cluster)
points(k4$centers, col="blue", pch=15)
```

The metric
```{r}
km$tot.withinss
k4$tot.withinss
```

> Q. Let's try different number of k (centers) from 1 to 30 and see what the best result is.

```{r}
ans <- NULL
for(i in 1:30) {
  ans <- c(ans, kmeans(x, centers=i)$tot.withinss)
}
```

```{r}
plot(ans, typ="o")
```

**Note:** K-means will apply a clustering structure that you specify, even if it is not supported by the data. i.e. It will give you what you ask for. A scree plot can show the point where the slope changes sharply is the most "effective"(?) value for k 

## Hierarchical Clustering

The main function for Hierarchical clustering is called `hclust()`. 

Unlike `kmeans()` (which does everything for you), you cannot just pass your raw input data into `hclust()`. It needs a distance matrix, like the one returned by the `dist()` function.

```{r}
d <- dist(x)
hc <- hclust(d)
plot(hc)
```

To extract our cluster membership vector from an `hclust()` result object we have to "cut" our tree at a given height to yield separate "groups"/"branches".

```{r}
plot(hc)
abline(h=8, col="red", lty=2)
```

To do this, we use the `cutree()` function on our `hclust()` object.

```{r}
grps <- cutree(hc, h=8)
grps
```

```{r}
table(grps, km$cluster)
```

## PCA of UK food data

Import the datasets on food consumption in the UK

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```

> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
dim(x)
```

One solution to set the row names is to do it by hand, one by one. 

```{r}
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
dim(x)
```

A better way to do this is to set the row names to the first column with `read.csv()`

```{r}
x <- read.csv(url, row.names=1)
x
```

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

The second, using `read.csv()` directly. The first approach will delete a column everyone time the code is run.

### Spotting major differences and trends

It is difficult to see, even for a small 17D dataset

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

### Pairs plots and heatmaps
```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)
```

```{r}
library(pheatmap)
pheatmap( as.matrix(x) )
```

## PCA to the Rescue

The main PCA function in "base R" is called `prcomp()`. This function wants the transpose of our food data as input (i.e. the foods as columns, and the countries as rows)

```{r}
pca <- prcomp(t(x))
```

```{r}
summary(pca)
```

```{r}
attributes(pca)
```

To make one of our main PCA result figures, we use `pca$x`for the scores along our new principal components (PCs). This is called "PC plot" or "score plot" or "ordination plot".

```{r}
pca$x

my_cols <- c("orange", "red", "blue", "darkgreen")
```


```{r}
library(ggplot2)

ggplot(pca$x) +
  aes(PC1, PC2) +
  geom_point(col=my_cols)
```

The second major result figure is called a "loadings plot", or "variable contributions plot", or "weight plot". 

```{r}
ggplot(pca$rotation) +
  aes(x = PC1, y = reorder(rownames(pca$rotation), PC1)) +
  geom_col()
```

